services:
  # Embedding service - runs once to populate Weaviate
  embed:
    build:
      context: .
      dockerfile: Dockerfile.embed
    container_name: iitm-chatbot-embed
    env_file:
      - .env
    volumes:
      - ./src:/app/src:ro
      - ./embed.py:/app/embed.py:ro
    profiles:
      - embed
    networks:
      - iitm-network

  # Cloudflare Worker dev server
  worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: iitm-chatbot-worker
    env_file:
      - .env
    ports:
      - "8787:8787"
    volumes:
      - ./worker.js:/app/worker.js:ro
      - ./wrangler.toml:/app/wrangler.toml:ro
      - ./static:/app/static:ro
      - ./.dev.vars:/app/.dev.vars:ro
    networks:
      - iitm-network

  # Local Weaviate instance (for self-hosted mode)
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.27.0
    container_name: iitm-weaviate
    command: --host 0.0.0.0 --port 8080 --scheme http --write-timeout=900s --read-timeout=900s
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-ollama'
      ENABLE_MODULES: 'text2vec-ollama'
      OLLAMA_API_ENDPOINT: 'http://ollama:11434'
      MODULES_CLIENT_TIMEOUT: '600s'
    volumes:
      - weaviate_data:/var/lib/weaviate
    profiles:
      - local
    networks:
      - iitm-network
    depends_on:
      - ollama

  # Ollama for local embeddings
  ollama:
    image: ollama/ollama:latest
    container_name: iitm-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    profiles:
      - local
    networks:
      - iitm-network

volumes:
  weaviate_data:
  ollama_data:

networks:
  iitm-network:
    driver: bridge
